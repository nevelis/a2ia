# Fixed: CUDA Out of Memory Error

## What Happened

You got `torch.OutOfMemoryError` because the model you tried to load was too large for 12GB VRAM.

**Your GPU:** NVIDIA RTX 3500 Ada (12GB VRAM)  
**Problem:** Trying to load unquantized Mixtral 8x7B (~40GB model into 12GB VRAM)

## The Fix: Use One of These Commands

### ‚úÖ Option 1: Mistral 7B (RECOMMENDED - Easy & Fast)

```bash
# Start vLLM with Mistral 7B
./vllm_mistral7b.sh

# Or manually:
vllm serve mistralai/Mistral-7B-Instruct-v0.2 \
  --dtype half \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.90 \
  --port 8000
```

**Then start A2IA with:**
```bash
a2ia-cli --backend vllm --model mistralai/Mistral-7B-Instruct-v0.2
```

**Why this works:**
- ‚úÖ Mistral 7B uses only ~6-7GB VRAM
- ‚úÖ Leaves 5GB headroom
- ‚úÖ Fast inference (60-80 tokens/sec)
- ‚úÖ Great quality
- ‚úÖ Good tool calling support

---

### ‚ö†Ô∏è Option 2: Mixtral 8x7B (AWQ Quantized - Advanced)

```bash
# Start vLLM with quantized Mixtral
./vllm_mixtral_awq.sh

# Or manually:
vllm serve TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ \
  --quantization awq \
  --dtype half \
  --max-model-len 2048 \
  --gpu-memory-utilization 0.85 \
  --port 8000
```

**Then start A2IA with:**
```bash
a2ia-cli --backend vllm --model mistralai/Mixtral-8x7B-Instruct-v0.1
```

**Why this might still be tight:**
- ‚ö†Ô∏è Uses ~10-11GB VRAM (very close to limit)
- ‚ö†Ô∏è Smaller context (2K tokens vs 8K)
- ‚ö†Ô∏è May still OOM under load
- ‚úÖ More powerful than Mistral 7B
- ‚úÖ AWQ quantization makes it possible

**If this still fails ‚Üí Use Option 1 instead**

---

### üîÑ Option 3: Just Use Ollama

vLLM is great but Ollama is easier for local development:

```bash
# Pull Mistral with Ollama
ollama pull mistral:7b-instruct

# Use with A2IA
a2ia-cli --backend ollama --model mistral:7b-instruct
```

**Advantages:**
- ‚úÖ Handles memory automatically
- ‚úÖ No manual configuration needed
- ‚úÖ Works out of the box
- ‚úÖ Great for development

---

## Why The Original Command Failed

You ran:
```bash
vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1  # ‚Üê This is ~40GB uncompressed!
```

**Problems:**
1. ‚ùå No quantization ‚Üí Full FP16 weights (~40GB)
2. ‚ùå Model doesn't fit in 12GB VRAM
3. ‚ùå Immediate OOM error

**What you needed:**
```bash
vllm serve TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ  # ‚Üê AWQ quantized (~10GB)
  --quantization awq  # ‚Üê Enable 4-bit compression
```

---

## Quick Reference

### Safe Model (Mistral 7B):
```bash
./vllm_mistral7b.sh
a2ia-cli --backend vllm --model mistralai/Mistral-7B-Instruct-v0.2
```

### Advanced Model (Mixtral 8x7B):
```bash
./vllm_mixtral_awq.sh
a2ia-cli --backend vllm --model mistralai/Mixtral-8x7B-Instruct-v0.1
```

### Monitor GPU:
```bash
watch -n 1 nvidia-smi
```

### Check vLLM Status:
```bash
./check_vllm.sh
```

---

## Memory Usage Comparison

| Model | VRAM Used | Context | Speed | Fits 12GB? |
|-------|-----------|---------|-------|------------|
| Mistral 7B (FP16) | ~6-7GB | 8K | 60-80 tok/s | ‚úÖ Easy |
| Mixtral 8x7B (AWQ) | ~10-11GB | 2-4K | 30-40 tok/s | ‚ö†Ô∏è Tight |
| Mixtral 8x7B (Full) | ~40GB | N/A | N/A | ‚ùå No |

---

## My Recommendation

**Start with Mistral 7B (`./vllm_mistral7b.sh`)**

It's:
- Reliable and fast
- Great quality
- Easy on your GPU
- Perfect for development

Once you're comfortable, try Mixtral AWQ if you need more power.

---

**Status:** Ready to try again! üöÄ

