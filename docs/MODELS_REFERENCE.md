# A2IA Models Reference

**Status:** ‚úÖ Both models fixed and working  
**Date:** 2025-10-27

---

## Available Models

### 1. `a2ia-qwen` (Recommended)

**Base:** `qwen2.5:7b`  
**Tool Support:** ‚úÖ Native tool calling  
**Template:** ChatML (`<|im_start|>`, `<|im_end|>`)  
**VRAM:** ~4-5GB  
**Best For:** General development with tool calling

**Create:**
```bash
./create_qwen_model.sh
```

**Use:**
```bash
# Default - works great with tools
a2ia-cli --model a2ia-qwen

# With debug
a2ia-cli --model a2ia-qwen --debug
```

**Why use this:**
- ‚úÖ Native tool calling support (no --react needed)
- ‚úÖ Fast and efficient
- ‚úÖ Great quality responses
- ‚úÖ Works perfectly with MCP tools

---

### 2. `a2ia-mistral`

**Base:** `mistral:7b-instruct-q4_K_M`  
**Tool Support:** ‚ùå No native tool calling (use --react)  
**Template:** Mistral Instruct (`[INST]...[/INST]`)  
**VRAM:** ~4-5GB  
**Best For:** General chat, --react mode

**Create:**
```bash
./create_mistral_model.sh
```

**Use:**
```bash
# Basic chat (no tool calling)
a2ia-cli --model a2ia-mistral

# With ReAct for tools
a2ia-cli --model a2ia-mistral --react
```

**Why use this:**
- ‚úÖ Good general responses
- ‚úÖ Works with --react mode
- ‚ö†Ô∏è Needs --react flag for tools
- ‚ö†Ô∏è Slower than Qwen for tool-heavy tasks

---

## Quick Comparison

| Feature | a2ia-qwen | a2ia-mistral |
|---------|-----------|--------------|
| Tool Calling | ‚úÖ Native | ‚ö†Ô∏è --react only |
| Speed | ‚ö° Fast | ‚ö° Fast |
| Quality | üåü Excellent | üåü Excellent |
| VRAM | 4-5GB | 4-5GB |
| Best For | Development | Chat/ReAct |
| Recommended | ‚úÖ Yes | ‚ö†Ô∏è Backup |

---

## Template Details

### Qwen Template (ChatML)

```
<|im_start|>system
{{ .System }}
<|im_end|>
<|im_start|>user
{{ .Prompt }}
<|im_end|>
<|im_start|>assistant
```

**Supports:**
- System prompts ‚úÖ
- Tool calling ‚úÖ
- Multi-turn conversations ‚úÖ

### Mistral Template (Instruct)

```
[INST] {{ .System }} {{ .Prompt }} [/INST]
```

**Supports:**
- System prompts ‚úÖ
- Tool calling ‚ùå
- Multi-turn conversations ‚úÖ

---

## Testing

### Test a2ia-qwen:

```bash
# Basic test
ollama run a2ia-qwen "What is 2+2?"

# With A2IA CLI
a2ia-cli --model a2ia-qwen
> List files in the workspace
```

### Test a2ia-mistral:

```bash
# Basic test
ollama run a2ia-mistral "What is 2+2?"

# With A2IA CLI (ReAct mode for tools)
a2ia-cli --model a2ia-mistral --react
> List files in the workspace
```

---

## Troubleshooting

### "model not found"

**Solution:** Create the model first:
```bash
./create_qwen_model.sh
# or
./create_mistral_model.sh
```

### "does not support tools" (Mistral)

**Solution:** Use --react flag:
```bash
a2ia-cli --model a2ia-mistral --react
```

### Qwen giving weird responses

**Problem:** Wrong template (Llama format instead of ChatML)  
**Solution:** Recreate with correct template:
```bash
./create_qwen_model.sh
```

### Mistral not responding properly

**Problem:** Wrong template format  
**Solution:** Recreate with correct template:
```bash
./create_mistral_model.sh
```

---

## Recommendation

**For A2IA Development:** Use `a2ia-qwen`

```bash
a2ia-cli --model a2ia-qwen
```

**Why:**
- Native tool calling (no --react workaround)
- Excellent quality
- Fast performance
- Works perfectly with MCP tools
- Less complexity

**For vLLM Comparison:**
- Both models use ~4-5GB VRAM (vs vLLM's 11.5GB failure)
- Both work reliably on 12GB GPU
- Native Ollama quantization and memory management

---

## Files

```
Modelfile-qwen              # Qwen with ChatML template
Modelfile-mistral           # Mistral with Instruct template
create_qwen_model.sh        # Create a2ia-qwen
create_mistral_model.sh     # Create a2ia-mistral
MODELS_REFERENCE.md         # This file
```

---

**Status:** All models working correctly ‚úÖ  
**Tested:** 2025-10-27  
**Hardware:** RTX 3500 (12GB VRAM)

