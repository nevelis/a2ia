#!/bin/bash
# vLLM Setup Script for Mixtral 8x7B on 12GB VRAM + 64GB RAM
# This script installs vLLM and provides commands to run Mixtral efficiently

set -e

echo "======================================================================"
echo "  A2IA vLLM Setup - Mixtral 8x7B Optimization"
echo "======================================================================"
echo ""
echo "Your Hardware:"
echo "  - VRAM: 12GB"
echo "  - RAM: 64GB"
echo ""
echo "Strategy:"
echo "  - Use quantized model (4-bit AWQ or GPTQ)"
echo "  - CPU offloading for layers that don't fit in VRAM"
echo "  - Optimized tensor parallelism disabled (single GPU)"
echo "======================================================================"
echo ""

# Check if we're in a virtual environment
if [[ -z "${VIRTUAL_ENV}" ]]; then
    echo "⚠️  Warning: Not in a virtual environment!"
    echo "   Consider activating a venv first:"
    echo "   python3 -m venv venv && source venv/bin/activate"
    echo ""
    read -p "Continue anyway? (y/N) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

echo "Step 1: Installing vLLM..."
echo "======================================================================"
pip install vllm

echo ""
echo "Step 2: Checking for CUDA..."
echo "======================================================================"
python3 -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}'); print(f'CUDA Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

echo ""
echo "======================================================================"
echo "✅ vLLM Installation Complete!"
echo "======================================================================"
echo ""
echo "Next Steps:"
echo ""
echo "1. Start vLLM with Mixtral 8x7B (quantized):"
echo ""
echo "   Option A - AWQ Quantized (Recommended for 12GB VRAM):"
echo "   vllm serve TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ \\"
echo "     --quantization awq \\"
echo "     --dtype half \\"
echo "     --max-model-len 4096 \\"
echo "     --gpu-memory-utilization 0.90 \\"
echo "     --enable-chunked-prefill \\"
echo "     --port 8000"
echo ""
echo "   Option B - GPTQ Quantized (Alternative):"
echo "   vllm serve TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ \\"
echo "     --quantization gptq \\"
echo "     --dtype half \\"
echo "     --max-model-len 4096 \\"
echo "     --gpu-memory-utilization 0.90 \\"
echo "     --port 8000"
echo ""
echo "   Option C - Full FP16 with CPU Offloading (May be slow):"
echo "   vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 \\"
echo "     --dtype half \\"
echo "     --max-model-len 2048 \\"
echo "     --gpu-memory-utilization 0.95 \\"
echo "     --swap-space 16 \\"
echo "     --cpu-offload-gb 40 \\"
echo "     --port 8000"
echo ""
echo "2. Once vLLM is running, start A2IA CLI in a new terminal:"
echo ""
echo "   a2ia-cli-vllm"
echo ""
echo "   Or with custom model name:"
echo ""
echo "   a2ia-cli-vllm --model mistralai/Mixtral-8x7B-Instruct-v0.1"
echo ""
echo "======================================================================"
echo "Performance Tips:"
echo "======================================================================"
echo ""
echo "- AWQ quantization gives best quality/performance balance on 12GB"
echo "- Reduce --max-model-len if you run out of memory"
echo "- Monitor GPU memory with: watch -n 1 nvidia-smi"
echo "- CPU offloading works but is much slower than pure GPU"
echo "- Consider using smaller model if performance is poor:"
echo "    - mistralai/Mistral-7B-Instruct-v0.2 (fits easily in 12GB)"
echo "    - mistralai/Mixtral-8x22B-Instruct-v0.1 (too large, skip)"
echo ""
echo "======================================================================"
echo "Troubleshooting:"
echo "======================================================================"
echo ""
echo "If vLLM fails to start:"
echo "  1. Check CUDA installation: nvidia-smi"
echo "  2. Try smaller max-model-len (e.g., 2048 instead of 4096)"
echo "  3. Reduce gpu-memory-utilization to 0.85"
echo "  4. Use quantized models (AWQ/GPTQ) instead of full precision"
echo ""
echo "If tool calling doesn't work:"
echo "  1. Mixtral doesn't natively support OpenAI function calling"
echo "  2. You may need to use ReAct prompting instead"
echo "  3. Try: a2ia-cli-vllm --react"
echo ""
echo "======================================================================"

